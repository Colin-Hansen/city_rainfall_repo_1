---
title: "City of Calgary Rainfall Data Notebook"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(printr)
library(ggplot2)
library(reshape2)
library(pander)


# added 12-DEC-2020
library(dplyr)
library(tidyr)
library(purrr)
library(lubridate)
library(scales)
library(rio)
library(readr)
library(knitr)
library(tinytex)

panderOptions('knitr.auto.asis', FALSE)
```



```{r p-match function, include=FALSE}


# create (if necessary) 'data_out' directory
if(dir.exists('data_out') == FALSE) dir.create('data_out')


# >>>>> p_match_Function <<<<<<<<<<<<<

  # function to search subdirectories for matching files
  p_match_Function <- function(files,name) {

  # strip the site names from the raw file names in each subfolder
  
  temp_names<-strtrim(dir(files),3)
 
  # check to see if the 'name' passed to the function exists in the trimmed file names
  if(name %in% temp_names)  {
    
 
  # the name of the matched file
  file_match_char<-list.files(files,pattern = name)
  #print(file_match_char)
  
  # the following 2 lines of code are not used here ( **BUT KEEP FOR NOW - the FILE APPEND MAY BE NEEDED ELSEWHERE **)
  #  file_match<-as.data.frame(file_match_char)
  #  if(file.exists(outfile_1) == FALSE) write_csv(file_match,outfile_1) else write_csv(file_match,outfile_1,append = TRUE)
  
  # the full path pf the matched file
  file_match_path<-file.path(files,file_match_char)
  #print(file_match_path)
  
   # reference the full path and the input file name using file.path
  infile<-file_match_path
  
  # read raw rainfall data using 'read.table'
  #rainfall_data<-read.table(infile,sep="\t",header=FALSE,skip=3,stringsAsFactors = FALSE)
  
  # read raw rainfall data using 'read_tsv'
  rainfall_data<-read_tsv(infile,col_names = FALSE,trim_ws = TRUE,skip = 3)
  
  colnames(rainfall_data)<-c("Date","Hour.Minute.Seconds","Rainfall")
  print(str(rainfall_data))
  
  #
  # create a column of Site.Name data
  Site.Name<-rep(name,nrow(rainfall_data))
  
  # get the discrete date - time components
  Year<-year(rainfall_data$Date)
  Month<-month(rainfall_data$Date)
  Day<-mday(rainfall_data$Date)
  Hour<-hour(rainfall_data$Hour.Minute.Seconds)
  Minute<-minute(rainfall_data$Hour.Minute.Seconds)  
  DOY<-yday(rainfall_data$Date)
  #Cumulative.Rainfall<-cumsum(rainfall_data$Rainfall)
  
  # column bind the data and the date info
  #temp_df<-cbind(Site.Name,rainfall_data,Date.Time,Year,Month,Day,Hour,Minute,DOY)
  temp_df<-cbind(Site.Name,rainfall_data,Year,Month,Day,Hour,Minute,DOY)
  #  print(head(temp_df))
  
  # create a directory to save the .CSV files
  CSV_master_folder_5_min<-paste0('data_out',"/MASTER_CSV_5_min")
 
  # create (if necessary) 'data_out/MASTER_CSV_5_min' directory
  if(dir.exists(CSV_master_folder_5_min) == FALSE) dir.create(CSV_master_folder_5_min)
  
  # create a file name to save the files for each site
  CSV_file_name<-paste0(name,"_5_min.csv")
  CSV_outfile<-file.path(CSV_master_folder_5_min, CSV_file_name)
  
  # write the results to a temporary .csv file so it can be appended to the original Pass #1 file
  temp_outfile<-file.path('data_out','temp_CSV.csv')
  
  # write the table without column names, since it will be appended  
  write.table(temp_df,temp_outfile, sep=",", row.names=FALSE,col.names = FALSE)
  
  # write to .csv file
  if(file.exists(CSV_outfile) == FALSE) write.table(temp_df, CSV_outfile, sep=",", row.names=FALSE,col.names = TRUE) 
  else file.append(CSV_outfile,temp_outfile)

    }   
    
  else {
    
    print ("No filename match in that subfolder")
  }  
  
  
  }  # Close of p_match_Function

  # pre-work before calling the
  
  
  # read the site names
  site_names<-read.table('Site Names/Site_Names_and_Ground_Elevations.csv',sep=",",header=TRUE,stringsAsFactors = FALSE)
  
  # extract a vector of the site names
  site_names<-as.character(site_names$Site.Name)
  

  # get the names of the raw data subfolders
  subfolder_names<-dir('data_in',full.names = TRUE)
  subfolder_names<-as.list(subfolder_names)
  # 
  # # specify which years of raw data to read
  first<-1
  last<-2 # do not use 26 ** that folder has Calgary International data
  
   subfolder_names_temp<-subfolder_names[first:last]
  
  #for (i in seq_along(site_names_2)) {

  for (i in 1:1) {

     name<-site_names[i]
     lapply(subfolder_names_temp, p_match_Function, name)

}

# *************************************************************



```

